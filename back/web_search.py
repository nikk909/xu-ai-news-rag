"""è”ç½‘æœç´¢åŠŸèƒ½"""
import requests
from bs4 import BeautifulSoup
import logging
import json
import urllib.parse

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebSearcher:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def search_duckduckgo(self, query, max_results=3):
        """ä½¿ç”¨DuckDuckGoæœç´¢ï¼ˆå…è´¹ï¼Œæ— éœ€APIå¯†é’¥ï¼‰"""
        try:
            # DuckDuckGo Instant Answer API
            api_url = f"https://api.duckduckgo.com/?q={urllib.parse.quote(query)}&format=json&no_html=1&skip_disambig=1"
            response = self.session.get(api_url, timeout=5)  # å‡å°‘è¶…æ—¶æ—¶é—´åˆ°5ç§’
            response.raise_for_status()
            data = response.json()
            
            results = []
            
            # å¤„ç†Instant Answerç»“æœ
            if data.get('AbstractText'):
                results.append({
                    'title': data.get('Heading', query),
                    'content': data.get('AbstractText', ''),
                    'link': data.get('AbstractURL', ''),
                    'source': 'DuckDuckGo',
                    'rank': 1
                })
            
            # å¤„ç†Related Topics
            for idx, topic in enumerate(data.get('RelatedTopics', [])[:max_results-1], 2):
                if isinstance(topic, dict) and 'Text' in topic:
                    results.append({
                        'title': topic.get('FirstURL', '').split('/')[-1] if topic.get('FirstURL') else f'ç›¸å…³ç»“æœ {idx}',
                        'content': topic.get('Text', ''),
                        'link': topic.get('FirstURL', ''),
                        'source': 'DuckDuckGo',
                        'rank': idx
                    })
            
            if results:
                logger.info(f"DuckDuckGoæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                return results[:max_results]
        except Exception as e:
            logger.warning(f"DuckDuckGoæœç´¢å¤±è´¥: {e}")
        
        return None
    
    def search_baidu_html(self, query, max_results=3):
        """ä½¿ç”¨ç™¾åº¦æœç´¢"""
        try:
            # å°è¯•ä½¿ç”¨ç™¾åº¦æœç´¢çš„å…¬å¼€æ¥å£
            search_url = f"https://www.baidu.com/s?wd={urllib.parse.quote(query)}"
            response = self.session.get(search_url, timeout=5)  # å‡å°‘è¶…æ—¶æ—¶é—´åˆ°5ç§’
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            results = []
            
            # å°è¯•å¤šç§å¯èƒ½çš„ç™¾åº¦æœç´¢ç»“æœé€‰æ‹©å™¨
            # ç™¾åº¦æœç´¢ç»“æœå¯èƒ½æœ‰ä¸åŒçš„HTMLç»“æ„
            selectors = [
                ('div', {'class': 'result'}),
                ('div', {'class': 'c-container'}),
                ('div', {'class': 'result-op'}),
                ('div', {'id': lambda x: x and 'result' in x.lower()}),
            ]
            
            result_divs = []
            for tag, attrs in selectors:
                result_divs = soup.find_all(tag, attrs)
                if result_divs:
                    logger.info(f"ä½¿ç”¨é€‰æ‹©å™¨ {tag} {attrs} æ‰¾åˆ° {len(result_divs)} ä¸ªç»“æœ")
                    break
            
            if not result_divs:
                # å°è¯•æ›´é€šç”¨çš„æ–¹æ³•ï¼šæŸ¥æ‰¾åŒ…å«é“¾æ¥çš„div
                result_divs = soup.find_all('div', class_=lambda x: x and ('result' in x.lower() or 'container' in x.lower()))
            
            for idx, div in enumerate(result_divs[:max_results], 1):
                try:
                    # å°è¯•å¤šç§æ–¹å¼æŸ¥æ‰¾æ ‡é¢˜
                    title_elem = div.find('h3') or div.find('h2') or div.find('a', class_=lambda x: x and 'title' in x.lower())
                    if not title_elem:
                        title_elem = div.find('a')
                    
                    # å°è¯•å¤šç§æ–¹å¼æŸ¥æ‰¾é“¾æ¥
                    link_elem = div.find('a', href=True)
                    if not link_elem and title_elem:
                        link_elem = title_elem
                    
                    # å°è¯•å¤šç§æ–¹å¼æŸ¥æ‰¾å†…å®¹æ‘˜è¦
                    content_elem = (
                        div.find('span', class_=lambda x: x and ('abstract' in x.lower() or 'content' in x.lower())) or
                        div.find('div', class_=lambda x: x and ('abstract' in x.lower() or 'content' in x.lower())) or
                        div.find('p', class_=lambda x: x and ('abstract' in x.lower() or 'content' in x.lower()))
                    )
                    
                    title = title_elem.get_text().strip() if title_elem else f"æœç´¢ç»“æœ {idx}"
                    link = link_elem.get('href', '') if link_elem else ''
                    content = content_elem.get_text().strip() if content_elem else ''
                    
                    # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œå°è¯•ä»divä¸­æå–æ–‡æœ¬
                    if not content:
                        all_text = div.get_text()
                        if title_elem:
                            title_text = title_elem.get_text()
                            content = all_text.replace(title_text, '').strip()[:200]
                    
                    if title:
                        results.append({
                            'title': title,
                            'content': content[:500] if content else f'å…³äº"{query}"çš„æœç´¢ç»“æœ',
                            'link': link,
                            'source': 'ç™¾åº¦æœç´¢',
                            'rank': idx
                        })
                except Exception as e:
                    logger.warning(f"è§£ææœç´¢ç»“æœé¡¹å¤±è´¥: {e}")
                    continue
            
            if results:
                logger.info(f"ç™¾åº¦æœç´¢HTMLè§£ææˆåŠŸï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
                return results[:max_results]
        except Exception as e:
            logger.warning(f"ç™¾åº¦æœç´¢HTMLè§£æå¤±è´¥: {e}")
        
        return None
    
    def search(self, query, max_results=3):
        """é€šç”¨æœç´¢æ¥å£ - ä¼˜å…ˆä½¿ç”¨çœŸå®æœç´¢ï¼Œå¤±è´¥åˆ™è¿”å›ç©ºåˆ—è¡¨ï¼ˆä¸è¿”å›è™šæ‹Ÿå†…å®¹ï¼‰"""
        logger.info(f"ğŸŒ å¼€å§‹è”ç½‘æœç´¢: query='{query}', max_results={max_results}")
        
        # æ–¹æ³•1: å°è¯•DuckDuckGoï¼ˆå…è´¹APIï¼‰
        logger.info("ğŸ” å°è¯•æ–¹æ³•1: DuckDuckGo API")
        results = self.search_duckduckgo(query, max_results)
        if results:
            logger.info(f"âœ… DuckDuckGoæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
        else:
            logger.warning("âŒ DuckDuckGoæœç´¢å¤±è´¥æˆ–æ— ç»“æœ")
        
        # æ–¹æ³•2: å°è¯•ç™¾åº¦HTMLè§£æ
        logger.info("ğŸ” å°è¯•æ–¹æ³•2: ç™¾åº¦HTMLè§£æ")
        results = self.search_baidu_html(query, max_results)
        if results:
            logger.info(f"âœ… ç™¾åº¦æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} æ¡ç»“æœ")
            return results
        else:
            logger.warning("âŒ ç™¾åº¦æœç´¢å¤±è´¥æˆ–æ— ç»“æœ")
        
        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ˆä¸è¿”å›è™šæ‹Ÿå†…å®¹ï¼‰
        logger.warning(f"âŒ æ‰€æœ‰è”ç½‘æœç´¢æ–¹æ³•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºç»“æœï¼ˆä¸è¿”å›è™šæ‹Ÿå†…å®¹ï¼‰")
        return []

