import faiss
import numpy as np
from sentence_transformers import SentenceTransformer, CrossEncoder
import pickle
import os
from pathlib import Path
# ä½¿ç”¨sentence-transformersç›´æ¥å®ç°ï¼Œä¸ä¾èµ–langchain
import logging

# å°è¯•å¯¼å…¥win32apiï¼ˆWindowsç³»ç»Ÿï¼‰
try:
    import win32api
    HAS_WIN32API = True
except ImportError:
    HAS_WIN32API = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SimpleTextSplitter:
    """ç®€å•çš„æ–‡æœ¬åˆ†å‰²å™¨"""
    def __init__(self, chunk_size=500, chunk_overlap=50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_text(self, text):
        """åˆ†å‰²æ–‡æœ¬"""
        if not text:
            return []
        
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + self.chunk_size
            chunk = text[start:end]
            
            # å°è¯•åœ¨å¥å·ã€æ¢è¡Œç¬¦ç­‰ä½ç½®åˆ†å‰²
            if end < text_length:
                # å‘åæŸ¥æ‰¾åˆ†å‰²ç‚¹
                for sep in ['\n\n', '\n', 'ã€‚', '.', 'ï¼', '!', 'ï¼Ÿ', '?']:
                    last_sep = chunk.rfind(sep)
                    if last_sep > self.chunk_size * 0.5:  # è‡³å°‘ä¿ç•™50%çš„å†…å®¹
                        chunk = chunk[:last_sep + len(sep)]
                        end = start + len(chunk)
                        break
            
            chunks.append(chunk.strip())
            start = end - self.chunk_overlap  # é‡å 
            
            if start >= text_length:
                break
        
        return chunks if chunks else [text]


class KnowledgeBase:
    def __init__(self, db_path='instance/faiss_index'):
        self.db_path = Path(db_path)
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        # åµŒå…¥æ¨¡å‹
        logger.info("åŠ è½½åµŒå…¥æ¨¡å‹: all-MiniLM-L6-v2")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # é‡æ’æ¨¡å‹
        logger.info("åŠ è½½é‡æ’æ¨¡å‹: cross-encoder/ms-marco-MiniLM-L-6-v2")
        try:
            from sentence_transformers import CrossEncoder
            self.rerank_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            logger.info("é‡æ’æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            logger.warning(f"é‡æ’æ¨¡å‹åŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•ä½¿ç”¨sentence-transformerç‰ˆæœ¬")
            try:
                # å¦‚æœcross-encoderå¤±è´¥ï¼Œå°è¯•sentence-transformerç‰ˆæœ¬
                self.rerank_model = SentenceTransformer('sentence-transformers/ms-marco-MiniLM-L-6-v2')
                logger.info("ä½¿ç”¨sentence-transformerç‰ˆæœ¬çš„é‡æ’æ¨¡å‹")
            except Exception as e2:
                logger.warning(f"sentence-transformerç‰ˆæœ¬ä¹ŸåŠ è½½å¤±è´¥: {e2}ï¼Œä½¿ç”¨åµŒå…¥æ¨¡å‹ä»£æ›¿")
                self.rerank_model = None
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = SimpleTextSplitter(
            chunk_size=500,
            chunk_overlap=50
        )
        
        # åˆå§‹åŒ–FAISS
        self.index = None
        self.documents = []
        self.load_index()
    
    def load_index(self):
        """åŠ è½½FAISSç´¢å¼•"""
        index_file = self.db_path / 'index.faiss'
        docs_file = self.db_path / 'documents.pkl'
        
        logger.info(f"å°è¯•åŠ è½½ç´¢å¼•: index_file={index_file.absolute()}, exists={index_file.exists()}")
        logger.info(f"å°è¯•åŠ è½½æ–‡æ¡£: docs_file={docs_file.absolute()}, exists={docs_file.exists()}")
        
        # å¦‚æœæ ‡å‡†è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ–‡ä»¶åï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰
        if not index_file.exists():
            # æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰.faissæ–‡ä»¶
            faiss_files = list(self.db_path.glob('*.faiss'))
            logger.info(f"æŸ¥æ‰¾.faissæ–‡ä»¶: æ‰¾åˆ° {len(faiss_files)} ä¸ª")
            if faiss_files:
                index_file = faiss_files[0]
                logger.info(f"ä½¿ç”¨æ‰¾åˆ°çš„ç´¢å¼•æ–‡ä»¶: {index_file.name}")
        
        if not docs_file.exists():
            # æŸ¥æ‰¾ç›®å½•ä¸‹æ‰€æœ‰.pklæ–‡ä»¶
            pkl_files = list(self.db_path.glob('*.pkl'))
            logger.info(f"æŸ¥æ‰¾.pklæ–‡ä»¶: æ‰¾åˆ° {len(pkl_files)} ä¸ª")
            if pkl_files:
                docs_file = pkl_files[0]
                logger.info(f"ä½¿ç”¨æ‰¾åˆ°çš„æ–‡æ¡£æ–‡ä»¶: {docs_file.name}")
        
        if index_file.exists() and docs_file.exists():
            try:
                self.index = faiss.read_index(str(index_file.resolve()))
                with open(docs_file.resolve(), 'rb') as f:
                    self.documents = pickle.load(f)
                logger.info(f"åŠ è½½ç´¢å¼•æˆåŠŸï¼ŒåŒ…å« {len(self.documents)} æ¡æ–‡æ¡£")
            except Exception as e:
                logger.error(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}", exc_info=True)
                # å¦‚æœç´¢å¼•æ–‡ä»¶æŸåï¼Œå°è¯•ä»documents.pklé‡æ–°ç”Ÿæˆ
                if docs_file.exists():
                    logger.info("ç´¢å¼•æ–‡ä»¶æŸåï¼Œå°è¯•ä»documents.pklé‡æ–°ç”Ÿæˆç´¢å¼•")
                    try:
                        with open(docs_file.resolve(), 'rb') as f:
                            self.documents = pickle.load(f)
                        if len(self.documents) > 0:
                            self._rebuild_index_from_documents()
                            return
                    except Exception as e2:
                        logger.error(f"ä»documents.pklé‡æ–°ç”Ÿæˆç´¢å¼•å¤±è´¥: {e2}")
                self._create_new_index()
        elif docs_file.exists() and not index_file.exists():
            # åªæœ‰documents.pklï¼Œæ²¡æœ‰index.faissï¼Œå°è¯•é‡æ–°ç”Ÿæˆç´¢å¼•
            logger.warning(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½†æ–‡æ¡£æ–‡ä»¶å­˜åœ¨ï¼Œå°è¯•é‡æ–°ç”Ÿæˆç´¢å¼•")
            logger.warning(f"ç›®å½•å†…å®¹: {list(self.db_path.glob('*'))}")
            try:
                with open(docs_file.resolve(), 'rb') as f:
                    self.documents = pickle.load(f)
                if len(self.documents) > 0:
                    logger.info(f"ä» {len(self.documents)} æ¡æ–‡æ¡£é‡æ–°ç”Ÿæˆç´¢å¼•")
                    self._rebuild_index_from_documents()
                else:
                    logger.warning("æ–‡æ¡£æ–‡ä»¶ä¸ºç©ºï¼Œåˆ›å»ºæ–°ç´¢å¼•")
                    self._create_new_index()
            except Exception as e:
                logger.error(f"åŠ è½½documents.pklå¤±è´¥: {e}")
                self._create_new_index()
        else:
            logger.warning(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: index.faiss={index_file.exists()}, documents.pkl={docs_file.exists()}")
            logger.warning(f"ç›®å½•å†…å®¹: {list(self.db_path.glob('*'))}")
            self._create_new_index()
    
    def _create_new_index(self):
        """åˆ›å»ºæ–°ç´¢å¼•"""
        # all-MiniLM-L6-v2çš„ç»´åº¦æ˜¯384
        dimension = 384
        self.index = faiss.IndexFlatL2(dimension)
        self.documents = []
        logger.info(f"åˆ›å»ºæ–°ç´¢å¼•ï¼Œç»´åº¦: {dimension}")
    
    def _rebuild_index_from_documents(self):
        """ä»ç°æœ‰æ–‡æ¡£é‡æ–°ç”Ÿæˆç´¢å¼•"""
        if len(self.documents) == 0:
            logger.warning("æ–‡æ¡£ä¸ºç©ºï¼Œæ— æ³•é‡æ–°ç”Ÿæˆç´¢å¼•")
            self._create_new_index()
            return
        
        logger.info(f"å¼€å§‹ä» {len(self.documents)} æ¡æ–‡æ¡£é‡æ–°ç”Ÿæˆç´¢å¼•...")
        
        # åˆ›å»ºæ–°ç´¢å¼•
        dimension = 384
        self.index = faiss.IndexFlatL2(dimension)
        
        # æå–æ‰€æœ‰æ–‡æ¡£æ–‡æœ¬
        all_texts = [doc.get('text', '') for doc in self.documents]
        
        # ç”Ÿæˆå‘é‡
        logger.info("æ­£åœ¨ç”Ÿæˆå‘é‡...")
        embeddings = self.embedding_model.encode(all_texts, show_progress_bar=False, batch_size=32)
        embeddings = np.array(embeddings).astype('float32')
        
        # æ·»åŠ åˆ°ç´¢å¼•
        logger.info("æ·»åŠ åˆ°FAISSç´¢å¼•...")
        self.index.add(embeddings)
        
        # ä¿å­˜ç´¢å¼•
        logger.info("ä¿å­˜é‡æ–°ç”Ÿæˆçš„ç´¢å¼•...")
        try:
            self.save_index()
            logger.info(f"ç´¢å¼•é‡æ–°ç”ŸæˆæˆåŠŸï¼ŒåŒ…å« {len(self.documents)} æ¡æ–‡æ¡£")
        except Exception as e:
            logger.error(f"ä¿å­˜é‡æ–°ç”Ÿæˆçš„ç´¢å¼•å¤±è´¥: {e}")
            logger.warning("ç´¢å¼•å·²åœ¨å†…å­˜ä¸­å¯ç”¨ï¼Œå¯ä»¥æ­£å¸¸æœç´¢ï¼Œä½†é‡å¯åéœ€è¦é‡æ–°ç”Ÿæˆ")
            # å³ä½¿ä¿å­˜å¤±è´¥ï¼Œç´¢å¼•ä¹Ÿåœ¨å†…å­˜ä¸­å¯ç”¨ï¼Œä¸å½±å“æœç´¢åŠŸèƒ½
    
    def save_index(self):
        """ä¿å­˜ç´¢å¼•"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.db_path.mkdir(parents=True, exist_ok=True)
            
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼ˆPathå¯¹è±¡ä¼šè‡ªåŠ¨å¤„ç†ï¼‰
            index_file = self.db_path / 'index.faiss'
            docs_file = self.db_path / 'documents.pkl'
            
            logger.info(f"å‡†å¤‡ä¿å­˜ç´¢å¼•åˆ°: {index_file.absolute()}")
            logger.info(f"å‡†å¤‡ä¿å­˜æ–‡æ¡£åˆ°: {docs_file.absolute()}")
            logger.info(f"ç´¢å¼•å¤§å°: {self.index.ntotal if hasattr(self.index, 'ntotal') else 'N/A'}")
            logger.info(f"æ–‡æ¡£æ•°é‡: {len(self.documents)}")
            
            # ä¿å­˜ç´¢å¼•æ–‡ä»¶ - ä½¿ç”¨å­—ç¬¦ä¸²è·¯å¾„
            try:
                index_path_str = str(index_file.resolve())
                # ç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
                index_file.parent.mkdir(parents=True, exist_ok=True)
                
                # å°è¯•å†™å…¥ç´¢å¼•æ–‡ä»¶ - ä½¿ç”¨bytesæ¨¡å¼ç¡®ä¿ç¼–ç æ­£ç¡®
                logger.info(f"å‡†å¤‡å†™å…¥ç´¢å¼•æ–‡ä»¶ï¼Œè·¯å¾„: {index_path_str}")
                logger.info(f"ç´¢å¼•ç±»å‹: {type(self.index)}, å¤§å°: {self.index.ntotal if hasattr(self.index, 'ntotal') else 'N/A'}")
                
                # ä½¿ç”¨ç»å¯¹è·¯å¾„å­—ç¬¦ä¸²ï¼Œç¡®ä¿è·¯å¾„æ­£ç¡®
                abs_path_str = str(Path(index_path_str).absolute())
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                Path(abs_path_str).parent.mkdir(parents=True, exist_ok=True)
                
                # å†™å…¥ç´¢å¼•æ–‡ä»¶ - ä½¿ç”¨å¤šç§æ–¹æ³•ç¡®ä¿æˆåŠŸ
                import time
                saved = False
                
                # æ–¹æ³•1: ä½¿ç”¨ç»å¯¹è·¯å¾„
                try:
                    faiss.write_index(self.index, abs_path_str)
                    time.sleep(0.2)  # ç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥
                    if Path(abs_path_str).exists():
                        saved = True
                        logger.info(f"FAISSç´¢å¼•å·²ä¿å­˜åˆ°: {abs_path_str}")
                except Exception as write_error:
                    logger.warning(f"ç»å¯¹è·¯å¾„ä¿å­˜å¤±è´¥: {write_error}")
                
                # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ç›¸å¯¹è·¯å¾„
                if not saved:
                    try:
                        relative_path = str(index_file)
                        faiss.write_index(self.index, relative_path)
                        time.sleep(0.2)
                        if Path(relative_path).exists():
                            saved = True
                            abs_path_str = str(Path(relative_path).absolute())
                            logger.info(f"ä½¿ç”¨ç›¸å¯¹è·¯å¾„ä¿å­˜æˆåŠŸ: {relative_path}")
                    except Exception as rel_error:
                        logger.warning(f"ç›¸å¯¹è·¯å¾„ä¿å­˜ä¹Ÿå¤±è´¥: {rel_error}")
                
                # æ–¹æ³•3: å¦‚æœéƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨çŸ­è·¯å¾„åï¼ˆWindowsï¼‰
                if not saved and os.name == 'nt' and HAS_WIN32API:
                    try:
                        short_path = win32api.GetShortPathName(str(self.db_path))
                        short_index_path = Path(short_path) / 'index.faiss'
                        faiss.write_index(self.index, str(short_index_path))
                        time.sleep(0.2)
                        if short_index_path.exists():
                            saved = True
                            abs_path_str = str(short_index_path.absolute())
                            logger.info(f"ä½¿ç”¨çŸ­è·¯å¾„åä¿å­˜æˆåŠŸ: {short_index_path}")
                    except Exception as short_error:
                        logger.warning(f"çŸ­è·¯å¾„åä¿å­˜å¤±è´¥: {short_error}")
                
                if not saved:
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®°å½•è­¦å‘Šï¼Œç´¢å¼•åœ¨å†…å­˜ä¸­ä»ç„¶å¯ç”¨
                    logger.error("æ‰€æœ‰ä¿å­˜æ–¹æ³•éƒ½å¤±è´¥ï¼Œç´¢å¼•ä»…åœ¨å†…å­˜ä¸­å¯ç”¨")
                    logger.warning("è¿™å¯èƒ½æ˜¯ç”±äºè·¯å¾„ç¼–ç é—®é¢˜ï¼Œä½†æœç´¢åŠŸèƒ½ä»ç„¶å¯ç”¨")
                    # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è°ƒç”¨è€…çŸ¥é“ä¿å­˜å¤±è´¥ä½†å¯ä»¥ç»§ç»­ä½¿ç”¨
                    return  # æå‰è¿”å›ï¼Œä¸ç»§ç»­éªŒè¯
                
                # éªŒè¯æ–‡ä»¶
                check_path = Path(abs_path_str)
                if check_path.exists():
                    file_size = check_path.stat().st_size
                    logger.info(f"éªŒè¯æˆåŠŸ: ç´¢å¼•æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {file_size} å­—èŠ‚")
                else:
                    # ç­‰å¾…æ›´é•¿æ—¶é—´åå†æ¬¡æ£€æŸ¥
                    time.sleep(0.5)
                    if check_path.exists():
                        file_size = check_path.stat().st_size
                        logger.info(f"å»¶è¿ŸéªŒè¯æˆåŠŸ: ç´¢å¼•æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {file_size} å­—èŠ‚")
                    else:
                        logger.error(f"è­¦å‘Š: ç´¢å¼•æ–‡ä»¶ä¿å­˜åéªŒè¯ä¸å­˜åœ¨: {abs_path_str}")
                        logger.error(f"ç›®å½•å†…å®¹: {list(check_path.parent.glob('*'))}")
            except Exception as e:
                logger.error(f"ä¿å­˜FAISSç´¢å¼•å¤±è´¥: {e}")
                logger.warning("ç´¢å¼•åœ¨å†…å­˜ä¸­ä»ç„¶å¯ç”¨ï¼Œå¯ä»¥æ­£å¸¸æœç´¢")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç´¢å¼•åœ¨å†…å­˜ä¸­ä½¿ç”¨
                # è¿™æ ·å³ä½¿ä¿å­˜å¤±è´¥ï¼Œæœç´¢åŠŸèƒ½ä»ç„¶å¯ç”¨
            
            # ä¿å­˜æ–‡æ¡£æ–‡ä»¶
            try:
                docs_path_str = str(docs_file.resolve())
                with open(docs_path_str, 'wb') as f:
                    pickle.dump(self.documents, f)
                logger.info(f"æ–‡æ¡£æ•°æ®å·²ä¿å­˜åˆ°: {docs_path_str}")
            except Exception as e:
                logger.error(f"ä¿å­˜æ–‡æ¡£æ•°æ®å¤±è´¥: {e}", exc_info=True)
                raise
            
            logger.info(f"ä¿å­˜ç´¢å¼•æˆåŠŸï¼ŒåŒ…å« {len(self.documents)} æ¡æ–‡æ¡£")
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„ä¿å­˜äº†ï¼ˆä½¿ç”¨resolveåçš„è·¯å¾„ï¼‰
            index_resolved = index_file.resolve()
            docs_resolved = docs_file.resolve()
            
            if not index_resolved.exists():
                logger.error(f"é”™è¯¯: ç´¢å¼•æ–‡ä»¶ä¿å­˜åä¸å­˜åœ¨: {index_resolved}")
                logger.error(f"ç›®å½•å†…å®¹: {list(self.db_path.resolve().glob('*'))}")
            else:
                logger.info(f"éªŒè¯æˆåŠŸ: ç´¢å¼•æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {index_resolved.stat().st_size} å­—èŠ‚")
                
            if not docs_resolved.exists():
                logger.error(f"é”™è¯¯: æ–‡æ¡£æ–‡ä»¶ä¿å­˜åä¸å­˜åœ¨: {docs_resolved}")
            else:
                logger.info(f"éªŒè¯æˆåŠŸ: æ–‡æ¡£æ–‡ä»¶å­˜åœ¨ï¼Œå¤§å°: {docs_resolved.stat().st_size} å­—èŠ‚")
        except Exception as e:
            logger.error(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {e}", exc_info=True)
            import traceback
            traceback.print_exc()
            raise
    
    def add_documents(self, texts, metadata_list=None):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        if not texts:
            return
        
        if metadata_list is None:
            metadata_list = [{}] * len(texts)
        
        # åˆ†å‰²æ–‡æœ¬
        all_chunks = []
        all_metadata = []
        
        for text, metadata in zip(texts, metadata_list):
            chunks = self.text_splitter.split_text(text)
            for chunk in chunks:
                all_chunks.append(chunk)
                all_metadata.append(metadata)
        
        # ç”Ÿæˆå‘é‡
        embeddings = self.embedding_model.encode(all_chunks, show_progress_bar=False)
        embeddings = np.array(embeddings).astype('float32')
        
        # æ·»åŠ åˆ°ç´¢å¼•
        self.index.add(embeddings)
        
        # ä¿å­˜æ–‡æ¡£
        for chunk, metadata in zip(all_chunks, all_metadata):
            self.documents.append({
                'text': chunk,
                'metadata': metadata
            })
        
        self.save_index()
        logger.info(f"æ·»åŠ  {len(all_chunks)} ä¸ªæ–‡æ¡£å—åˆ°çŸ¥è¯†åº“")
    
    def search(self, query, top_k=10, similarity_threshold=0.3):
        """æœç´¢çŸ¥è¯†åº“"""
        logger.info(f"ğŸ” å¼€å§‹æœç´¢çŸ¥è¯†åº“: query='{query}', top_k={top_k}, threshold={similarity_threshold}")
        logger.info(f"ğŸ“š çŸ¥è¯†åº“æ–‡æ¡£æ€»æ•°: {len(self.documents)}")
        
        if len(self.documents) == 0:
            logger.warning("âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•æœç´¢")
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        logger.info("ğŸ”„ æ­£åœ¨ç”ŸæˆæŸ¥è¯¢å‘é‡...")
        query_embedding = self.embedding_model.encode([query])
        query_embedding = np.array(query_embedding).astype('float32')
        logger.info(f"âœ… æŸ¥è¯¢å‘é‡ç”Ÿæˆå®Œæˆï¼Œç»´åº¦: {query_embedding.shape}")
        
        # æœç´¢
        k = min(top_k, len(self.documents))
        if k == 0:
            logger.warning("âš ï¸ k=0ï¼Œæ— æ³•æœç´¢")
            return []
        
        logger.info(f"ğŸ” åœ¨FAISSç´¢å¼•ä¸­æœç´¢ï¼Œk={k}")
        distances, indices = self.index.search(query_embedding, k)
        logger.info(f"ğŸ“Š æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(indices[0])} ä¸ªå€™é€‰ç»“æœ")
        
        results = []
        filtered_count = 0
        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
            if idx < len(self.documents) and idx >= 0:
                # L2è·ç¦»è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
                # ä½¿ç”¨æ›´åˆç†çš„è·ç¦»è½¬æ¢ï¼šall-MiniLM-L6-v2çš„å…¸å‹è·ç¦»èŒƒå›´æ˜¯0-2
                max_distance = 2.0
                similarity = max(0, 1 - (distance / max_distance))
                
                logger.debug(f"ç»“æœ {i+1}: idx={idx}, distance={distance:.4f}, similarity={similarity:.4f}, threshold={similarity_threshold}")
                
                # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè®©æ›´å¤šç»“æœèƒ½å¤Ÿè¿”å›
                if similarity >= similarity_threshold:
                    doc = self.documents[idx].copy()
                    doc['similarity'] = float(similarity)
                    doc['rank'] = i + 1
                    results.append(doc)
                    logger.debug(f"âœ… ç»“æœ {i+1} é€šè¿‡é˜ˆå€¼è¿‡æ»¤: similarity={similarity:.4f}")
                else:
                    filtered_count += 1
                    logger.debug(f"âŒ ç»“æœ {i+1} æœªé€šè¿‡é˜ˆå€¼è¿‡æ»¤: similarity={similarity:.4f} < {similarity_threshold}")
        
        logger.info(f"ğŸ“ˆ æœç´¢ç»Ÿè®¡: æ€»å€™é€‰={len(indices[0])}, é€šè¿‡é˜ˆå€¼={len(results)}, è¢«è¿‡æ»¤={filtered_count}")
        
        # é‡æ’åºï¼ˆä½¿ç”¨é‡æ’æ¨¡å‹æå‡ç›¸å…³æ€§ï¼‰
        if len(results) > 0 and self.rerank_model and len(results) <= 50:  # åªå¯¹å‰50æ¡è¿›è¡Œé‡æ’ï¼Œé¿å…å¤ªæ…¢
            logger.info(f"ğŸ”„ å¼€å§‹é‡æ’åºï¼Œç»“æœæ•°: {len(results)}")
            try:
                doc_texts = [r['text'] for r in results]
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯CrossEncoder
                if isinstance(self.rerank_model, CrossEncoder):
                    # CrossEncoderç›´æ¥æ¥å—(query, document)å¯¹ï¼Œè¿”å›ç›¸å…³æ€§åˆ†æ•°
                    pairs = [[query, doc_text] for doc_text in doc_texts]
                    rerank_scores = self.rerank_model.predict(pairs)
                    # å°†numpyæ•°ç»„è½¬æ¢ä¸ºåˆ—è¡¨
                    if hasattr(rerank_scores, 'tolist'):
                        rerank_scores = rerank_scores.tolist()
                    elif isinstance(rerank_scores, np.ndarray):
                        rerank_scores = rerank_scores.tolist()
                    else:
                        rerank_scores = list(rerank_scores)
                else:
                    # å¦‚æœæ˜¯SentenceTransformerï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•
                    query_emb = self.rerank_model.encode([query])[0]
                    doc_embs = self.rerank_model.encode(doc_texts)
                    
                    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸ºé‡æ’åˆ†æ•°
                    rerank_scores = []
                    query_norm = np.linalg.norm(query_emb)
                    for doc_emb in doc_embs:
                        doc_norm = np.linalg.norm(doc_emb)
                        if query_norm > 0 and doc_norm > 0:
                            score = float(np.dot(query_emb, doc_emb) / (query_norm * doc_norm))
                        else:
                            score = 0.0
                        rerank_scores.append(score)
                
                # æ›´æ–°ç»“æœçš„é‡æ’åˆ†æ•°
                for i, result in enumerate(results):
                    result['rerank_score'] = float(rerank_scores[i])
                    # ä½¿ç”¨é‡æ’åˆ†æ•°å’ŒåŸå§‹ç›¸ä¼¼åº¦çš„åŠ æƒå¹³å‡ï¼ˆé‡æ’åˆ†æ•°æƒé‡æ›´é«˜ï¼‰
                    original_sim = result.get('similarity', 0)
                    result['final_score'] = 0.6 * float(rerank_scores[i]) + 0.4 * original_sim
                
                # æŒ‰æœ€ç»ˆåˆ†æ•°æ’åº
                results.sort(key=lambda x: x.get('final_score', x.get('similarity', 0)), reverse=True)
                logger.info(f"âœ… é‡æ’åºå®Œæˆï¼Œå…±å¤„ç† {len(results)} æ¡ç»“æœ")
                # æ˜¾ç¤ºå‰3æ¡ç»“æœçš„åˆ†æ•°
                for i, r in enumerate(results[:3]):
                    logger.info(f"  æ’å {i+1}: similarity={r.get('similarity', 0):.4f}, final_score={r.get('final_score', 0):.4f}")
            except Exception as e:
                logger.warning(f"é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹ç›¸ä¼¼åº¦æ’åº")
                import traceback
                traceback.print_exc()
                # å¦‚æœé‡æ’å¤±è´¥ï¼Œè‡³å°‘æŒ‰ç›¸ä¼¼åº¦æ’åº
                results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        else:
            # å¦‚æœæ²¡æœ‰é‡æ’ï¼Œè‡³å°‘æŒ‰ç›¸ä¼¼åº¦æ’åº
            results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
            logger.info(f"ğŸ“Š æœªä½¿ç”¨é‡æ’æ¨¡å‹ï¼ŒæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å› {len(results)} æ¡ç»“æœ")
        
        logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæœ€ç»ˆè¿”å› {len(results)} æ¡ç»“æœ")
        if len(results) > 0:
            logger.info(f"   æœ€é«˜ç›¸ä¼¼åº¦: {results[0].get('similarity', 0):.4f}")
            logger.info(f"   æœ€ä½ç›¸ä¼¼åº¦: {results[-1].get('similarity', 0):.4f}")
        
        return results
    
    def delete_documents_by_filename(self, filename):
        """æ ¹æ®æ–‡ä»¶ååˆ é™¤æ–‡æ¡£"""
        if not filename:
            return 0
        
        original_count = len(self.documents)
        # è¿‡æ»¤æ‰åŒ¹é…çš„æ–‡ä»¶å
        self.documents = [
            doc for doc in self.documents 
            if doc.get('metadata', {}).get('file_name') != filename
        ]
        deleted_count = original_count - len(self.documents)
        
        if deleted_count > 0:
            # é‡æ–°æ„å»ºç´¢å¼•
            logger.info(f"åˆ é™¤ {deleted_count} ä¸ªæ–‡æ¡£å—ï¼Œé‡æ–°æ„å»ºç´¢å¼•...")
            self._rebuild_index_from_documents()
            logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆï¼Œå‰©ä½™ {len(self.documents)} ä¸ªæ–‡æ¡£")
        
        return deleted_count
    
    def cleanup_missing_files(self, existing_files):
        """æ¸…ç†ä¸å­˜åœ¨çš„æ–‡ä»¶å¯¹åº”çš„æ–‡æ¡£
        existing_files: å­˜åœ¨çš„æ–‡ä»¶åé›†åˆ
        """
        if not existing_files:
            return 0
        
        original_count = len(self.documents)
        # åªä¿ç•™æ–‡ä»¶å­˜åœ¨çš„æ–‡æ¡£
        self.documents = [
            doc for doc in self.documents 
            if doc.get('metadata', {}).get('file_name') in existing_files
        ]
        deleted_count = original_count - len(self.documents)
        
        if deleted_count > 0:
            # é‡æ–°æ„å»ºç´¢å¼•
            logger.info(f"æ¸…ç† {deleted_count} ä¸ªä¸å­˜åœ¨çš„æ–‡ä»¶å¯¹åº”çš„æ–‡æ¡£å—ï¼Œé‡æ–°æ„å»ºç´¢å¼•...")
            self._rebuild_index_from_documents()
            logger.info(f"ç´¢å¼•é‡å»ºå®Œæˆï¼Œå‰©ä½™ {len(self.documents)} ä¸ªæ–‡æ¡£")
        
        return deleted_count
    
    def get_stats(self):
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_documents': len(self.documents),
            'index_size': self.index.ntotal if self.index else 0
        }

